{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cb7b12c"
      },
      "source": [
        "# Modificación de vídeos (cambio de cara) utilizando Generative Adversarial Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42e9ea52"
      },
      "source": [
        "<div style=\"background-color:#D9EEFF;color:black;padding:2%;\">\n",
        "<h2>Enunciado del caso práctico</h2>\n",
        "\n",
        "En este caso práctico, se propone al alumno el uso de Generative Adversarial Networks (GANs) para modificar un vídeo e intercambiar la cara de una persona.\n",
        "\n",
        "Para este caso concreto se prompone el uso de un técnica/arquitectura de Generative Adversarial Network conocida como [StyleGan2](https://github.com/NVlabs/stylegan2) y más concretamente un desarrollo influenciado por esta Red Neuronal Artificial Profunda que se denomina [Stitch in Time](https://stitch-time.github.io/).\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contexto adicional sobre Generative Adversarial Networks\n",
        "\n",
        "Desde el año en el que surgieron este tipo de Redes Neuronales Artificiales Profundas no han parado de publicarse [aplicaciones muy interesantes y modificaciones de su arquitectura inicial](https://arxiv.org/abs/2008.02793).\n",
        "\n",
        "En términos generales, podríamos clasificar las GANs en varias categorías:\n",
        "\n",
        "**1. GANs no condicionales y condicionales**\n",
        "\n",
        "* **No condicionales**: El generador convierte una entrada de ruido en una imagen falsa, y el discriminador diferencia entre imágenes reales e imágenes falsas. No hay señales de control adicionales para guiar el proceso de generación. Un ejemplo muy interesante es: https://thispersondoesnotexist.com/\n",
        "\n",
        "* **Condicionales**: El generador recibe una señal de control adicional como entrada, que podría ser otra imagen, texto o una etiqueta categórica. El objetivo del generador es producir salidas que correspondan a la señal de control proporcionada. Por ejemplo, si la señal de control es una etiqueta de categoría, el generador debería producir una imagen que pertenezca a esa categoría específica. El discriminador también tiene en cuenta la señal de control para diferenciar entre imágenes reales e imágenes generadas.\n",
        "\n",
        "**2. Tipos de GANs Basados en su Arquitectura**\n",
        "\n",
        "* **GAN Original**: Introducido por Goodfellow et al., establece la estructura básica de las GANs.\n",
        "* **DCGAN (Deep Convolutional GAN)**: Introduce capas convolucionales en las GANs para mejorar la calidad de las imágenes generadas.\n",
        "* **CoGAN (Coupled GANs)**: Utiliza múltiples GANs entrenadas juntas para mejorar la generación de imágenes.\n",
        "* **PgGAN (Progressive Growing GAN)**: Aumenta progresivamente la resolución de las imágenes generadas durante el entrenamiento para mejorar la calidad.\n",
        "* **StyleGAN**: Introduce un enfoque basado en estilos para la generación de imágenes, permitiendo un control más fino sobre las características generadas.\n",
        "\n",
        "**3. Aplicaciones de las GANs**\n",
        "* **Síntesis Semántica de Imágenes**: Conversión de representaciones semánticas editables por humanos a imágenes fotorrealistas.\n",
        "* **Traducción de Imágenes**: Traducción de imágenes de un dominio a otro.\n",
        "* **Restauración de Imágenes, Superresolución e Inpainting**: Transformación de distribuciones de imágenes para mejorar la calidad visual.\n",
        "* **Síntesis de Video**: Generación y manipulación de videos.\n",
        "* **Renderizado Neural**: Uso de redes neuronales para mejorar los procesos de renderizado gráfico.\n",
        "\n"
      ],
      "metadata": {
        "id": "0NPiLzG8O6ai"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "831d29b1"
      },
      "source": [
        "# Resolución del caso práctico"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Instalación de librerías externas"
      ],
      "metadata": {
        "id": "-V8dgd_BUeOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo primero que vamos a necesitar para resolver este caso práctico es descargar la Generative Adversarial Network del repositorio de Github: https://github.com/rotemtzaban/STIT y subir este repositorio a Google Drive después de aplicarle algunas configuraciones."
      ],
      "metadata": {
        "id": "Hdr-kpxER9bR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/drive/MyDrive/STIT-main/requirements.txt\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "ScQcgtCkUhD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7d620f6"
      },
      "source": [
        "## 2. División del vídeo en frames"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, debemos dividr el vídeo en un conjunto de imágenes fíjas que pueda editar la Red Neuronal Artificial y después volver a componer en forma de un vídeo nuevo.\n",
        "\n",
        "El vídeo que yo he creado tiene una duración de 7 segundos y 30 fps."
      ],
      "metadata": {
        "id": "-UggAevpSiJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i \"/content/drive/MyDrive/STIT-main/video_santi.mp4\" -vf \"scale=720:-1\" \"/content/drive/MyDrive/STIT-main/santi_frames/out%04d.png\""
      ],
      "metadata": {
        "id": "zR_poUNfjq7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fine-tuning del modelo"
      ],
      "metadata": {
        "id": "kjNFCLMHWCXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El entrenamiento de las Generative Adversarial Networks es un proceso extremadamente costoso y que requiere grandes recursos computacionales.\n",
        "\n",
        "Por este motivo, vamos a aplicar la técnica de re-entrenamiento (fine-tuning) que hemos presentado en secciones anteriores.\n",
        "\n",
        "En este caso el fine-tuning lo aplicamos sobre un modelo base ya entrenado con caras de personas."
      ],
      "metadata": {
        "id": "OP3Zs5O1WHER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/STIT-main/train.py --input_folder /content/drive/MyDrive/STIT-main/santi_frames \\\n",
        " --output_folder /content/drive/MyDrive/STIT-main/santi/train_results \\\n",
        " --run_name santi \\\n",
        " --num_pti_steps 3"
      ],
      "metadata": {
        "id": "dkxOGRD-WF3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Generación/Modificación del vídeo"
      ],
      "metadata": {
        "id": "Ej2wP15VXrPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLegados a este punto ya tenemos todo listo para utilizar nuestra Red Neuronal Artificial con Fine-tuning para realizar la modificación del vídeo."
      ],
      "metadata": {
        "id": "za_9VJf0TkHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generación de un vídeo donde modifica la edad\n",
        "!python /content/drive/MyDrive/STIT-main/edit_video.py --input_folder /content/drive/MyDrive/STIT-main/santi_frames \\\n",
        " --output_folder /content/drive/MyDrive/STIT-main/edits/santi3 \\\n",
        " --run_name santi2 \\\n",
        " --edit_name age \\\n",
        " --edit_range 8 8 1"
      ],
      "metadata": {
        "id": "Lrw-S4VgXxBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generación de un vídeo donde elimina la sonrisa\n",
        "!python /content/drive/MyDrive/STIT-main/edit_video_stitching_tuning.py --input_folder /content/drive/MyDrive/STIT-main/video_frames \\\n",
        " --output_folder /content/drive/MyDrive/STIT-main/edits/santi7 \\\n",
        " --run_name santi3 \\\n",
        " --edit_name smile \\\n",
        " --edit_range -3 -3 1 \\\n",
        " --outer_mask_dilation 50"
      ],
      "metadata": {
        "id": "XistqClIZABM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}