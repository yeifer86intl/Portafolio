{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3cb7b12c",
      "metadata": {
        "id": "3cb7b12c"
      },
      "source": [
        "# Redes Neuronales Recurrentes: Generación de texto con LSTMs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42e9ea52",
      "metadata": {
        "id": "42e9ea52"
      },
      "source": [
        "<div style=\"background-color:#D9EEFF;color:black;padding:2%;\">\n",
        "<h2>Enunciado del caso práctico</h2>\n",
        "\n",
        "En este caso práctico se propone al alumno la implementación de un programa que permita generar el titular de un artículo en el idioma Español.\n",
        "\n",
        "Para ello, debe implementarse un modelo generativo utilizando Redes Neuronales Recurrentes (LSTMs).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0d3c58f",
      "metadata": {
        "id": "c0d3c58f"
      },
      "source": [
        "<div style=\"background-color:#EEEEEE;color:black;padding:2%;\">\n",
        "<h2> Conjunto de datos </h2>\n",
        "    \n",
        "El conjunto de datos que debe utilizarse para este caso práctico esta compuesto por 1079 titulares escritos en español.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "831d29b1",
      "metadata": {
        "id": "831d29b1"
      },
      "source": [
        "# Resolución del caso práctico"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7d620f6",
      "metadata": {
        "id": "e7d620f6"
      },
      "source": [
        "## 1. Lectura del conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "753f78af",
      "metadata": {
        "id": "753f78af"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/titulares.txt\", encoding=\"latin-1\") as f:\n",
        "    titulares = f.read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "043a2e45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "043a2e45",
        "outputId": "ab50f973-64f9-450f-c0d4-d27b0f8c7c4c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Nueva ley de energía promete revolucionar el sector eléctrico',\n",
              " 'El cambio climático sigue siendo una amenaza global',\n",
              " 'Inversionistas buscan oportunidades en energías renovables',\n",
              " 'Aumenta la demanda de vehículos eléctricos',\n",
              " 'Vacunas contra COVID-19: ¿Cuándo estaremos todos protegidos?',\n",
              " 'El debate sobre las vacunas sigue dividiendo opiniones',\n",
              " 'Expertos en salud analizan la efectividad de las vacunas',\n",
              " 'Vacunación masiva contra el coronavirus en marcha',\n",
              " 'El mercado de criptomonedas se dispara a nuevas alturas',\n",
              " '¿Es Bitcoin la moneda del futuro?']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "titulares[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fabad183",
      "metadata": {
        "id": "fabad183"
      },
      "source": [
        "## 2. Preparación del conjunto de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1518b7e0",
      "metadata": {
        "id": "1518b7e0"
      },
      "source": [
        "### 2.1. Limpieza del conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05b82a2d",
      "metadata": {
        "id": "05b82a2d"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import unicodedata\n",
        "\n",
        "def preproc_texto(txt):\n",
        "    txt = \"\".join(c for c in txt if c not in string.punctuation).lower()\n",
        "    txt = unicodedata.normalize('NFKD', txt).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a80dbd2c",
      "metadata": {
        "id": "a80dbd2c"
      },
      "outputs": [],
      "source": [
        "dataset = [preproc_texto(t) for t in titulares]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5f11c51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5f11c51",
        "outputId": "498595f1-3289-4620-97fc-dd0dd31aaa59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nueva ley de energia promete revolucionar el sector electrico',\n",
              " 'el cambio climatico sigue siendo una amenaza global',\n",
              " 'inversionistas buscan oportunidades en energias renovables',\n",
              " 'aumenta la demanda de vehiculos electricos',\n",
              " 'vacunas contra covid19 cuando estaremos todos protegidos',\n",
              " 'el debate sobre las vacunas sigue dividiendo opiniones',\n",
              " 'expertos en salud analizan la efectividad de las vacunas',\n",
              " 'vacunacion masiva contra el coronavirus en marcha',\n",
              " 'el mercado de criptomonedas se dispara a nuevas alturas',\n",
              " 'es bitcoin la moneda del futuro']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "dataset[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c606c7a",
      "metadata": {
        "id": "3c606c7a"
      },
      "source": [
        "### 2.2. Tokenización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af54ee69",
      "metadata": {
        "id": "af54ee69"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "237f0870",
      "metadata": {
        "id": "237f0870"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizar_texto(tokenizer, dataset):\n",
        "  # Construccion del tokenizador\n",
        "  tokenizer.fit_on_texts(dataset)\n",
        "  total_palabras = len(tokenizer.word_index) + 1\n",
        "  # Tokenizacion del texto del conjunto de datos\n",
        "  dataset_tokens = []\n",
        "  for text in dataset:\n",
        "      titular_tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "      # Transformación del texto en ngramas\n",
        "      for i in range(1, len(titular_tokens)):\n",
        "          sec_n_gram = titular_tokens[:i+1]\n",
        "          dataset_tokens.append(sec_n_gram)\n",
        "  return dataset_tokens, total_palabras"
      ],
      "metadata": {
        "id": "uvDKJ0R4W5fn"
      },
      "id": "uvDKJ0R4W5fn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_tokens, total_palabras = tokenizar_texto(tokenizer, dataset)"
      ],
      "metadata": {
        "id": "uWtaEh1FcnoT"
      },
      "id": "uWtaEh1FcnoT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Generación de las etiquetas para el conjunto de datos\n",
        "\n",
        "Cuando entrenamos una red neuronal recurrente (RNN), como una LSTM, para predecir palabras en un texto, estamos abordando una tarea de procesamiento de lenguaje natural (NLP) conocida como \"modelado de lenguaje\". El objetivo principal es que la red pueda aprender a predecir la próxima palabra en una secuencia de texto dada una serie de palabras anteriores.\n",
        "\n",
        "Para entrenar esta red, necesitamos etiquetar nuestro conjunto de datos de entrenamiento de manera adecuada. En lugar de convertir directamente el texto en n-gramas, lo que hacemos es crear pares de entrada y salida. Aquí está cómo funciona con un ejemplo:\n",
        "\n",
        "Ejemplo:\n",
        "\n",
        "Supongamos que tenemos la siguiente frase:\n",
        "\n",
        "\n",
        "```\n",
        "El gato está en el tejado.\n",
        "```\n",
        "\n",
        "\n",
        "Creamos pares de entrada y salida de la siguiente manera:\n",
        "\n",
        "```\n",
        "Entrada: \"El\", Salida: \"gato\"\n",
        "Entrada: \"El gato\", Salida: \"está\"\n",
        "Entrada: \"El gato está\", Salida: \"en\"\n",
        "Entrada: \"El gato está en\", Salida: \"el\"\n",
        "Entrada: \"El gato está en el\", Salida: \"tejado\"\n",
        "```\n",
        "\n",
        "Cada \"Entrada\" es una secuencia de palabras en orden y la \"Salida\" es la siguiente palabra en esa secuencia. De esta manera, le estamos diciendo a la red que, dado un contexto previo de palabras, intente predecir cuál será la siguiente palabra. Esto permite que el modelo capture patrones y relaciones entre las palabras en función de su contexto.\n",
        "\n",
        "Es importante señalar que, a pesar de que estamos trabajando con secuencias de palabras, estamos manteniendo el orden de las palabras para capturar el contexto y la dependencia entre ellas.\n",
        "\n",
        "Una vez que tengas estos pares de entrada y salida etiquetados, puedes utilizarlos para entrenar una red neuronal, como una LSTM, para que pueda predecir la siguiente palabra en una secuencia de texto en función del contexto anterior. Esta es una técnica común en el procesamiento de lenguaje natural y se utiliza en aplicaciones como la generación de texto automático, corrección gramatical y muchas otras tareas relacionadas con el lenguaje natural."
      ],
      "metadata": {
        "id": "bfIKSwJKZNEm"
      },
      "id": "bfIKSwJKZNEm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a36f41",
      "metadata": {
        "id": "79a36f41"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow.keras.utils as ku"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da407280",
      "metadata": {
        "id": "da407280"
      },
      "outputs": [],
      "source": [
        "def generación_etiquetas(dataset_tokens, total_palabras):\n",
        "    # Obtengo el tamaño de la mayor secuencia del conjunto de datos\n",
        "    max_sequence_len = max([len(text) for text in dataset_tokens])\n",
        "    # Aplico padding al resto de secuencias hasta el tamaño máximo\n",
        "    dataset_tokens = np.array(pad_sequences(dataset_tokens, maxlen=max_sequence_len, padding='pre'))\n",
        "    # Genero las etiquetas para cada ejemplo del conjunto de datos\n",
        "    X_train, y_train = dataset_tokens[:,:-1],dataset_tokens[:,-1]\n",
        "    y_train = ku.to_categorical(y_train, num_classes=total_palabras)\n",
        "    return X_train, y_train, max_sequence_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf636cce",
      "metadata": {
        "id": "cf636cce"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, max_sequence_len = generación_etiquetas(dataset_tokens, total_palabras)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "393cedb5",
      "metadata": {
        "id": "393cedb5"
      },
      "source": [
        "## 3. Construcción del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42ff7bca",
      "metadata": {
        "id": "42ff7bca"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a93348e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a93348e",
        "outputId": "44a57163-2138-4c98-f1dc-ea102d3e296d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 14, 10)            12170     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               44400     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1217)              122917    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 179487 (701.12 KB)\n",
            "Trainable params: 179487 (701.12 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def create_model(max_sequence_len, total_palabras):\n",
        "    input_len = max_sequence_len - 1\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add Input Embedding Layer\n",
        "    model.add(Embedding(total_palabras, 10, input_length=input_len))\n",
        "\n",
        "    # Add Hidden Layer 1 - LSTM Layer\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    # Add Output Layer\n",
        "    model.add(Dense(total_palabras, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model(max_sequence_len, total_palabras)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3619d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb3619d6",
        "outputId": "040bde41-8037-495c-d25f-7263212db29b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Epoch 2/100\n",
            "Epoch 3/100\n",
            "Epoch 4/100\n",
            "Epoch 5/100\n",
            "Epoch 6/100\n",
            "Epoch 7/100\n",
            "Epoch 8/100\n",
            "Epoch 9/100\n",
            "Epoch 10/100\n",
            "Epoch 11/100\n",
            "Epoch 12/100\n",
            "Epoch 13/100\n",
            "Epoch 14/100\n",
            "Epoch 15/100\n",
            "Epoch 16/100\n",
            "Epoch 17/100\n",
            "Epoch 18/100\n",
            "Epoch 19/100\n",
            "Epoch 20/100\n",
            "Epoch 21/100\n",
            "Epoch 22/100\n",
            "Epoch 23/100\n",
            "Epoch 24/100\n",
            "Epoch 25/100\n",
            "Epoch 26/100\n",
            "Epoch 27/100\n",
            "Epoch 28/100\n",
            "Epoch 29/100\n",
            "Epoch 30/100\n",
            "Epoch 31/100\n",
            "Epoch 32/100\n",
            "Epoch 33/100\n",
            "Epoch 34/100\n",
            "Epoch 35/100\n",
            "Epoch 36/100\n",
            "Epoch 37/100\n",
            "Epoch 38/100\n",
            "Epoch 39/100\n",
            "Epoch 40/100\n",
            "Epoch 41/100\n",
            "Epoch 42/100\n",
            "Epoch 43/100\n",
            "Epoch 44/100\n",
            "Epoch 45/100\n",
            "Epoch 46/100\n",
            "Epoch 47/100\n",
            "Epoch 48/100\n",
            "Epoch 49/100\n",
            "Epoch 50/100\n",
            "Epoch 51/100\n",
            "Epoch 52/100\n",
            "Epoch 53/100\n",
            "Epoch 54/100\n",
            "Epoch 55/100\n",
            "Epoch 56/100\n",
            "Epoch 57/100\n",
            "Epoch 58/100\n",
            "Epoch 59/100\n",
            "Epoch 60/100\n",
            "Epoch 61/100\n",
            "Epoch 62/100\n",
            "Epoch 63/100\n",
            "Epoch 64/100\n",
            "Epoch 65/100\n",
            "Epoch 66/100\n",
            "Epoch 67/100\n",
            "Epoch 68/100\n",
            "Epoch 69/100\n",
            "Epoch 70/100\n",
            "Epoch 71/100\n",
            "Epoch 72/100\n",
            "Epoch 73/100\n",
            "Epoch 74/100\n",
            "Epoch 75/100\n",
            "Epoch 76/100\n",
            "Epoch 77/100\n",
            "Epoch 78/100\n",
            "Epoch 79/100\n",
            "Epoch 80/100\n",
            "Epoch 81/100\n",
            "Epoch 82/100\n",
            "Epoch 83/100\n",
            "Epoch 84/100\n",
            "Epoch 85/100\n",
            "Epoch 86/100\n",
            "Epoch 87/100\n",
            "Epoch 88/100\n",
            "Epoch 89/100\n",
            "Epoch 90/100\n",
            "Epoch 91/100\n",
            "Epoch 92/100\n",
            "Epoch 93/100\n",
            "Epoch 94/100\n",
            "Epoch 95/100\n",
            "Epoch 96/100\n",
            "Epoch 97/100\n",
            "Epoch 98/100\n",
            "Epoch 99/100\n",
            "Epoch 100/100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x796623f889a0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model.fit(X_train, y_train, epochs=100, verbose=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e01b3f1a",
      "metadata": {
        "id": "e01b3f1a"
      },
      "source": [
        "## 4. Predicción de nuevos ejemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49e09420",
      "metadata": {
        "id": "49e09420"
      },
      "outputs": [],
      "source": [
        "def generar_texto(prompt, num_palabras, modelo, max_sequence_len):\n",
        "    for _ in range(num_palabras):\n",
        "        # Pre-procesamiento del prompt\n",
        "        prompt_proc = preproc_texto(prompt)\n",
        "        prompt_proc = tokenizer.texts_to_sequences([prompt_proc])[0]\n",
        "        prompt_proc = pad_sequences([prompt_proc], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "        # Prediccion de la siguiente palabra\n",
        "        predict = model.predict(prompt_proc, verbose=0)\n",
        "        predicted = np.argmax(predict, axis=1)\n",
        "\n",
        "        # Concatenamos la palabra generada al prompt\n",
        "        siguiente_palabra = \"\"\n",
        "        for palabra,index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                siguiente_palabra = palabra\n",
        "                break\n",
        "        prompt += \" \" + siguiente_palabra\n",
        "    return prompt.title()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbf2f593",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbf2f593",
        "outputId": "f210f2d7-4992-4c5d-81c6-8bb9948356c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La Ciberseguridad En La Era Digital Protegiendo La Informacion Sensible\n"
          ]
        }
      ],
      "source": [
        "print(generar_texto(\"La Ciberseguridad\", 8, model, max_sequence_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "101346f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "101346f2",
        "outputId": "d7ed6385-da23-403d-95fe-5573def18c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La Inteligencia Artificial En La Educacion\n"
          ]
        }
      ],
      "source": [
        "print(generar_texto(\"La Inteligencia Artificial\", 3, model, max_sequence_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab514b69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab514b69",
        "outputId": "643005b2-3f86-456b-b407-e8b6ed5b2a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El Futuro De La Educacion Impulsado Por La Ia\n"
          ]
        }
      ],
      "source": [
        "print(generar_texto(\"El futuro\", 7, model, max_sequence_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24e35be0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24e35be0",
        "outputId": "61b8b6df-f9f2-421e-9419-c0afbafd22a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El Cambio Climatico Sigue Siendo Una Amenaza Global\n"
          ]
        }
      ],
      "source": [
        "print(generar_texto(\"El cambio climatico\", 5, model, max_sequence_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c612261",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c612261",
        "outputId": "4b74de87-51d2-4b7d-f512-ec9afbdfcf1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La Nueva Ley Y La Sostenibilidad\n"
          ]
        }
      ],
      "source": [
        "print(generar_texto(\"La nueva ley\", 3, model, max_sequence_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc3ae70a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc3ae70a",
        "outputId": "bd9d6c32-1f23-4377-d8e6-3fdae02d1d66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El Mundo De La Educacion Impulsado Por La\n"
          ]
        }
      ],
      "source": [
        "print(generar_texto(\"El mundo\", 6, model, max_sequence_len))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}